# ATLASky-AI

## Multi-Domain 4D Spatiotemporal Knowledge Graph Verification System

ATLASky-AI is a domain-adaptable verification system for 4D Spatiotemporal Knowledge Graphs (STKGs) that combines physics-based constraints with multi-modal verification to detect and prevent:

- **Content Hallucination**: Fabricated facts not grounded in reality
- **ST-Inconsistency**: Violations of physical laws (spatial/temporal)
- **Semantic Drift**: Facts that deviate from domain ontology

---

## Demo â€” See It In Action

### Video Walkthrough

https://github.com/rodaw92/AtlaSkI-AI/raw/main/demo/atlaskyai_demo.mp4

> **[â–¶ï¸ Download/watch the full demo video](https://github.com/rodaw92/AtlaSkI-AI/raw/main/demo/atlaskyai_demo.mp4)** â€” Methodology overview â†’ Fact verification â†’ AAIC monitoring

---

### Dashboard Overview

<p align="center">
  <img src="demo/dashboard_overview.png" alt="ATLASky-AI Dashboard" width="100%">
</p>

The interactive dashboard shows the STKG formalization **G = (V, E, O, T, Î¨)** with live knowledge graph metrics and physics-based consistency predicates:

<p align="center">
  <img src="demo/physics_predicates.png" alt="Physics Predicates Ïˆ_s, Ïˆ_t, Î¨" width="100%">
</p>

### Five-Module Verification Pipeline

Each candidate fact passes through 5 modules sequentially (Mâ‚â†’Mâ‚…), each computing dual metrics. Early termination occurs when cumulative confidence exceeds the global threshold.

<p align="center">
  <img src="demo/methodology_modules.png" alt="Five Verification Modules" width="100%">
</p>

### How Verification Works â€” Step by Step

The system processes each fact through a **three-stage pipeline** (Preprocessing â†’ LLM Extraction â†’ TruthFlow Verification), then classifies it as **ST** (spatiotemporal â€” has valid coordinates and timestamp) or **SEM** (semantic-only â€” missing or invalid coordinates). The final decision is **Accept**, **Review**, or **Reject** based on cumulative module confidence vs. the global threshold Î˜.

Below are two example cases showing exactly what the system checks and why a fact passes or fails.

---

#### âœ… Case 1: High-Quality Fact â†’ ACCEPT

**Input text:**
> *"Installation completed in Bay 7. Blade Gamma measurement: 3.02 mm on leading edge. Tolerance check passed."*

**Why this is high quality â€” what the system sees:**

| Property | Value | Why it matters |
|---|---|---|
| Entity | `TurbineBlade_Gamma` | Valid entity â€” exists in ontology class `Blade` âœ“ |
| Relationship | `hasMeasurement` | Valid relationship â€” defined in ontology R_o âœ“ |
| Location | Bay 7 â†’ (40.0, 20.0, 0.0) | Known facility location, mapped to real coordinates âœ“ |
| Timestamp | `2026-02-25T12:30:33Z` | Valid ISO 8601 UTC timestamp âœ“ |
| LLM Confidence | **High (1.0)** | Complete info: numbers + location + timestamp + detailed text |

**How each module scores it:**

| Module | Score | Threshold | Activated? | What it checked |
|---|---|---|---|---|
| Mâ‚ (LOV) | 0.700 | 0.70 | âœ… Yes | Subject `Blade` âˆˆ ontology, relation `hasMeasurement` âˆˆ ontology, object `InspectionMeasurement` âˆˆ ontology â†’ Metricâ‚ = 1.0 |
| Mâ‚‚ (POV) | 0.610 | 0.70 | âŒ No | Some terms match standards but not enough to exceed threshold |
| Mâ‚ƒ (MAV) | 1.000 | 0.65 | âœ… Yes | Ïˆ_s = 1 (no bilocation), Ïˆ_t = 1 (travel time feasible), Kinematic OK, Process OK |

**Result:** C = (0.700 Ã— 0.25 + 1.000 Ã— 0.20) / (0.25 + 0.20) = **0.800** â‰¥ Î˜ = 0.650 â†’ **ACCEPT** âœ… (early termination at Mâ‚ƒ, Mâ‚„ and Mâ‚… skipped)

<p align="center">
  <img src="demo/result_high_quality.png" alt="High Quality Accepted" width="90%">
</p>

---

#### âŒ Case 2: Low-Quality Fact â†’ REJECT

**Input text:**
> *"Blade part inspected. Measured approximately 3.5. Seems okay."*

**Why this is low quality â€” what the system sees:**

| Property | Value | Why it fails |
|---|---|---|
| Entity | `Unknown_6959` | Not in ontology â€” unknown entity class âœ— |
| Relationship | `linkedTo` / `contains` | Invalid â€” not defined in ontology R_o âœ— |
| Inspection tool | `UnknownTool_123` | Fabricated â€” not in standard tool list âœ— |
| Timestamp | `202X-12-5` | Unparseable â€” not valid ISO 8601 âœ— |
| Location | Missing coordinate axis | Incomplete spatial data âœ— |
| Fact Type | **SEM** (semantic-only) | Can't verify physics because coordinates are invalid |
| LLM Confidence | **Low (0.6)** | Vague text, no precise numbers, no clear location |

**How each module scores it:**

| Module | Score | Threshold | Activated? | What it detected |
|---|---|---|---|---|
| Mâ‚ (LOV) | 0.533 | 0.70 | âŒ No | `Unknown_6959` not in entity classes â†’ Metricâ‚ = 0.33 (only 1 of 3 structural checks pass) |
| Mâ‚‚ (POV) | 0.130 | 0.70 | âŒ No | `linkedTo` not in standard terminology, `UnknownTool_123` not a recognized tool â†’ Metricâ‚ = 0.20 |
| Mâ‚ƒ (MAV) | 1.000 | 0.65 | â¬œ Neutral | SEM fact â†’ physics N/A, neutral score does not count toward confidence |
| Mâ‚„ (WSV) | 0.350 | 0.60 | âŒ No | No corroborating evidence found in knowledge graph â†’ Metricâ‚ = 0.00 |
| Mâ‚… (ESV) | 0.518 | 0.65 | âŒ No | Low similarity to known facts â†’ statistical outlier detected |

**Result:** No modules activated â†’ C = **0.000** < Î˜ âˆ’ Îµ = 0.550 â†’ **REJECT** âŒ (all 5 modules executed, none reached activation threshold)

<p align="center">
  <img src="demo/result_low_quality.png" alt="Low Quality Rejected" width="90%">
</p>

---

#### Summary: What Makes a Fact Pass or Fail?

| Check | High Quality (Accept) | Low Quality (Reject) |
|---|---|---|
| **Entity class** | Known (`Blade`, `EngineSet`) | Unknown (`Unknown_XXXX`) |
| **Relationship type** | Valid (`hasMeasurement`, `containsBlade`) | Invalid (`linkedTo`, `contains`) |
| **Timestamp** | Valid ISO 8601 (`2026-02-25T12:30:33Z`) | Unparseable (`202X-12-5`) |
| **Coordinates** | Complete (x, y, z from facility map) | Missing or incomplete |
| **Inspection tool** | Standard (`3D_Scanner_Unit`) | Fabricated (`UnknownTool_123`) |
| **LLM confidence** | High (1.0) â€” precise text with all details | Low (0.6) â€” vague, missing info |
| **Fact type** | ST (spatiotemporal) | SEM (semantic-only) |
| **Physics check** | Ïˆ_s = 1, Ïˆ_t = 1 (consistent) | N/A (can't check without valid coordinates) |
| **Decision** | **C = 0.80 â‰¥ 0.65 â†’ Accept** | **C = 0.00 < 0.55 â†’ Reject** |

---

### AAIC Adaptive Monitoring

The Autonomous Adaptive Intelligence Cycle (AAIC) monitors module performance via CGR-CUSUM and adapts weights, thresholds, and alpha parameters when distribution shifts are detected.

<p align="center">
  <img src="demo/aaic_monitoring.png" alt="AAIC CGR-CUSUM Monitoring" width="100%">
</p>

### CLI Demo

Run `python3 test_verification_demo.py` to see all quality cases (high, medium, spatial, low) processed through the verification pipeline from the command line.

---

## System Architecture

### 4D STKG Formalization

ATLASky-AI operates on a formal 4D STKG defined as **G = (V, E, O, T, Î¨)** where:

- **V**: Versioned entities with immutable attributes and mutable state
- **E**: Directed edges representing relationships
- **O = (C, R_o, A)**: Domain ontology with entity classes, relation types, and attributes
- **T: (V âˆª E) â†’ â„Â³ Ã— â„**: Maps entities/relations to spatiotemporal coordinates (x,y,z,t)
- **Î¨: (V âˆª E) â†’ {0,1}**: Physical consistency predicate combining spatial (Ïˆ_s) and temporal (Ïˆ_t) consistency

### Physics-Based Predicates

- **Ïˆ_s (Spatial Consistency)**: Prevents co-location violations â€” same entity cannot exist at two separated locations within the same time window
- **Ïˆ_t (Temporal Consistency)**: Enforces velocity and travel-time constraints â€” travel time must be physically feasible given distance and maximum velocity
- **Î¨ = Ïˆ_s âˆ§ Ïˆ_t**: Combined predicate ensuring full physical consistency

### Three-Stage Pipeline

1. **Stage 1 â€” Data Preprocessing**: Normalizes heterogeneous raw data RD into structured format RD' (OCR, spell correction, terminology standardization via ontology O, temporal alignment to UTC, spatial validation via facility maps)
2. **Stage 2 â€” LLM-Based Extraction**: Generates candidate facts D = L(RD'; P) using domain-specialized prompts with confidence-weighted output d_k = âŸ¨s, r, o, T(d_k), conf_kâŸ© where conf_k âˆˆ {high=1.0, medium=0.8, low=0.6}
3. **Stage 3 â€” TruthFlow Verification**: Validates candidates through Ranked Multi-Modal Verification (RMMVe) with Autonomous Adaptive Intelligence Cycle (AAIC)

### Five-Module Verification Pipeline (RMMVe)

Each module M_i computes confidence through two complementary metrics:

**S_i(d_k) = conf_k Ã— [Î±_i Â· Metricâ‚ + (1âˆ’Î±_i) Â· Metricâ‚‚]**

| Module | Full Name | Primary Target | Dual Metrics | Cost |
|--------|-----------|----------------|--------------|------|
| Mâ‚ (LOV) | Lexical-Ontological Verification | Semantic Drift | Structural Compliance (Eq. 8) + Attribute Compliance (Eq. 9) | 5 ms |
| Mâ‚‚ (POV) | Protocol-Ontology Verification | Content Hallucination | Standard Terminology Match (Eq. 10) + Cross-Standard Consistency (Eq. 11) | 15 ms |
| Mâ‚ƒ (MAV) | Motion-Aware Verification | ST-Inconsistency | Temporal-Spatial Validity Ïˆ_s, Ïˆ_t (Eq. 12) + Physical Feasibility min(Kinematic, Process) (Eq. 13-16) | 50 ms |
| Mâ‚„ (WSV) | Web-Source Verification | Content Hallucination | Source Credibility (Eq. 17) + Cross-Source Agreement (Eq. 18) | 120 ms |
| Mâ‚… (ESV) | Embedding Similarity Verification | Semantic Drift + Hallucination | K-NN Cosine Similarity (Eq. 19) + Cluster Membership / GMM (Eq. 20) | 800 ms |

**Key mechanisms:**

- **Fact Type Classification**: Facts are classified as **ST** (spatiotemporal â€” has valid coordinates) or **SEM** (semantic-only). For SEM facts, MAV assigns neutral score Sâ‚ƒ=1.0 (physics not applicable).
- **Critical Module Veto**: For ST facts, if MAV score < Ï„_veto (healthcare: 0.5, aerospace: 0.30) â†’ immediate Reject regardless of other modules.
- **Early Termination**: When cumulative confidence C â‰¥ Î˜ (global threshold), remaining modules are skipped. For ST facts, early termination is suspended until Mâ‚ƒ has executed.
- **Three-Way Decision** (Eq. 23): Accept if C â‰¥ Î˜; Review if Î˜âˆ’Îµ â‰¤ C < Î˜ (Îµ=0.1); Reject if C < Î˜âˆ’Îµ.

### Autonomous Adaptive Intelligence Cycle (AAIC)

AAIC monitors per-module precision via **CGR-CUSUM** (Eq. 24):

**G_i(n) = max(0, G_i(n-1) + [p_i(n) âˆ’ Î¼_0 âˆ’ k])**

where k = 0.5Ïƒ (allowable slack), h = 5Ïƒ (alarm threshold). When G_i(n) â‰¥ h, three-level adaptation triggers:

- **Weight** (Eq. 25): `w_i â† w_i Ã— exp[âˆ’Î³ Â· G_i(t)]`, renormalise Î£w_i = 1 (Î³ = 0.01)
- **Threshold** (Eq. 26): `Î¸_i â† Î¸_i + Î· Â· sign(FPR_i âˆ’ FNR_i)` (Î· = 0.05)
- **Alpha** (Eq. 27): `Î±_i â† Î±_i + Î·' Â· âˆ‚L_i/âˆ‚Î±_i`, clip [0,1] (Î·' = 0.02)

### Defense-in-Depth Architecture

The five-layer verification achieves robustness through three principles:

1. **Independence**: Modules operate on distinct information sources (ontology, standards, physics, web, embeddings)
2. **Complementarity**: Modules target different error classes â€” ontology-compliant fabrications evade Mâ‚/Mâ‚ƒ but are caught by Mâ‚‚/Mâ‚„
3. **Redundancy**: Hallucination covered by both Mâ‚‚ (terminology) and Mâ‚„ (external evidence); Drift covered by both Mâ‚ (ontology) and Mâ‚… (embeddings)

### Domain Adaptation Protocol

Deploying in new domains requires configuring five components:

1. **Domain Ontology (O)**: Entity classes C, relation types R_o, attributes A (50â€“200 classes typical)
2. **Industry Standards (Mâ‚‚)**: STEP AP242 for aerospace, HL7 FHIR for healthcare, ISA-95 for manufacturing
3. **Physical Constraints (Mâ‚ƒ)**: Max velocities v_max, minimum process durations, facility geometry, veto threshold Ï„_veto
4. **Source Credibility (Mâ‚„)**: Credibility weights w_cred per source type (government > manufacturer > academic > news)
5. **Domain Embeddings (Mâ‚…)**: Sentence-transformers on â‰¥10K historical facts, quarterly retraining

## Key Features

- **Multi-Domain Support** â€” Aerospace, Healthcare, Aviation, CAD/Engineering
- **Physics-Based Verification** â€” Enforces Ïˆ_s (bilocation) and Ïˆ_t (velocity/travel-time) consistency
- **Three-Stage Pipeline** â€” Data Preprocessing â†’ LLM Extraction â†’ TruthFlow Verification
- **Adaptive Intelligence** â€” AAIC auto-adjusts w, Î¸, Î± via CGR-CUSUM monitoring
- **Honest Verification** â€” Real ontology checking, real standard terminology matching, real embedding similarity
- **ST/SEM Fact Classification** â€” Physics checks applied only to spatiotemporal facts
- **Critical Module Veto** â€” MAV can immediately reject physically impossible ST facts
- **Automatic STKG Integration** â€” Accepted facts added to knowledge graph
- **Interactive Visualization** â€” 7 dashboard tabs:
  - ğŸ“š Methodology â€” STKG formalization, physics predicates, error taxonomy, five-module pipeline
  - ğŸŒ Domain Configuration â€” Load/edit domain configs (ontology, standards, physics, credibility, embeddings)
  - ğŸ—‚ï¸ STKG Structure â€” Knowledge graph visualization, domain examples, ontology browser
  - ğŸ’  Verification Process â€” Three-stage pipeline with upload/processing capabilities
  - ğŸ”„ AAIC Monitoring â€” CGR-CUSUM tracking and parameter shift detection
  - ğŸ“Š Parameter Evolution â€” Weight, threshold, and alpha adaptation over time
  - ğŸ“œ Verification History â€” Complete audit trail of all verifications

## Getting Started

### Prerequisites

- Python 3.8+
- Dependencies listed in `requirements.txt`:
  - `streamlit>=1.24.0`
  - `pandas>=1.5.0`
  - `numpy>=1.23.0`
  - `matplotlib>=3.6.0`
  - `plotly>=5.14.0`

### Installation

```bash
pip install -r requirements.txt
```

### Running the Application

```bash
streamlit run app.py
```

The dashboard opens at `http://localhost:8501`.

## Code Structure

```
â”œâ”€â”€ app.py                          # Main Streamlit application (7 tabs)
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ knowledge_graph.py          # 4D STKG with physics predicates (Ïˆ_s, Ïˆ_t, Î¨)
â”‚   â”œâ”€â”€ ontology.py                 # Multi-domain ontology (16 entity classes, 11 relationships)
â”‚   â””â”€â”€ constants.py                # Physical params, veto thresholds, CUSUM params, standard terminologies
â”œâ”€â”€ verification/
â”‚   â”œâ”€â”€ rmmve.py                    # RMMVe: ST/SEM classification, veto, 3-way decision, early termination
â”‚   â”œâ”€â”€ modules.py                  # 5 modules (LOV, POV, MAV, WSV, ESV) with real dual-metric implementations
â”‚   â”œâ”€â”€ base.py                     # Base module: S_i = conf_k Ã— [Î±Â·M1 + (1âˆ’Î±)Â·M2]
â”‚   â”œâ”€â”€ domain_adapter.py           # Domain adaptation (5-component configuration)
â”‚   â””â”€â”€ defense_in_depth.py         # Defense-in-Depth analysis (independence, complementarity, redundancy)
â”œâ”€â”€ aaic/
â”‚   â””â”€â”€ aaic.py                     # CGR-CUSUM monitoring, FPR/FNR threshold, loss-gradient alpha
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ preprocessing.py            # Stage 1: RD â†’ RD' (OCR, spell correction, temporal alignment, spatial mapping)
â”‚   â”œâ”€â”€ llm_extraction.py           # Stage 2: D = L(RD'; P) with confidence {high:1.0, medium:0.8, low:0.6}
â”‚   â”œâ”€â”€ generators.py               # Test data with quality-specific issues (semantic, spatial, low)
â”‚   â””â”€â”€ quality_based_generator.py  # Raw text generation for honest testing
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ run_experiments.py          # Full experiment runner across 4 domains
â”‚   â”œâ”€â”€ quick_demo.py               # Quick CLI demo
â”‚   â”œâ”€â”€ datasets/                   # Domain-specific dataset generators
â”‚   â””â”€â”€ metrics/                    # Evaluation: Precision, Recall, F1, FPR
â”œâ”€â”€ visualization/                  # Plotly charts and Streamlit UI components
â”œâ”€â”€ utils/                          # CSS styles
â””â”€â”€ domains/                        # Domain configuration JSON files
```

## Usage

### Interactive Dashboard

```bash
streamlit run app.py
```

**Basic Workflow:**

1. **Generate Test Fact** (Sidebar): Select domain and quality level â†’ Click "ğŸ² Generate Test Fact"
2. **Run Verification** (Verification Process Tab): Stage 1 â†’ Stage 2 â†’ Stage 3
3. **View Results**: Decision (Accept/Review/Reject), fact type (ST/SEM), module scores, cumulative confidence

**Or Upload Your Own Data:**
- Stage 1: Upload TXT/JSON file â†’ Configure domain â†’ Run preprocessing
- Stage 2: Extract facts with domain-specialized LLM prompts
- Stage 3: Verify via RMMVe + AAIC and integrate accepted facts into STKG

### Command-Line Testing

```bash
# Verification pipeline demo (high, medium, spatial, low quality)
python3 test_verification_demo.py

# Quick experiment demo with metrics
python3 experiments/quick_demo.py

# Full experiments on specific or all datasets
python3 experiments/run_experiments.py --dataset manufacturing --num-facts 100
python3 experiments/run_experiments.py --all --num-facts 100

# Domain adaptation test
python3 test_domain_adaptation.py
```

Results are saved to `experiments/results/` as JSON with complete metrics and confusion matrices.

## License

This project is for demonstration purposes only.
